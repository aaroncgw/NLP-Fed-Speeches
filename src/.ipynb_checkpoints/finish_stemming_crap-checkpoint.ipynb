{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 733 entries, 0 to 734\n",
      "Data columns (total 5 columns):\n",
      "date       733 non-null datetime64[ns]\n",
      "speaker    733 non-null object\n",
      "title      733 non-null object\n",
      "link       733 non-null object\n",
      "text       733 non-null object\n",
      "dtypes: datetime64[ns](1), object(4)\n",
      "memory usage: 34.4+ KB\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "df = pickle.load( open( \"../data/all_fed_speeches\", \"rb\" ) )\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def tokenize(doc):\n",
    "#    return [snowball.stem(word) for word in word_tokenize(doc.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp = tokenize(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = list(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## building my own damn pipeline\n",
    "# tokenizng \n",
    "tokenized_docs= [doc.split() for doc in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenized_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "733"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase\n",
    "toc_doc_lower = [[word.lower() for word in doc] for doc in tokenized_docs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "punct = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(word, symbol_set):\n",
    "    return ''.join(char for char in word \n",
    "                    if char not in symbol_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_docs = [[remove_symbols(word, punct) for word in doc] \n",
    "                for doc in toc_doc_lower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toc_doc_lower[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "stops = set(stopwords.words('english'))\n",
    "doc_no_stops = [[word for word in doc if word not in stops] for doc in cleaned_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' I do not like the stemmer. I am getting many words that are the same basic words \n",
    "stemmer = SnowballStemmer('english)\n",
    "docs_stemmed = [[stemmer.stem(word) for word in doc] for doc in doc_no_stops]'''\n",
    "\n",
    "# tring lemmatizer\n",
    "lemmer = WordNetLemmatizer()\n",
    "docs_lemmed = [[lemmer.lemmatize(word) for word in doc] for doc in doc_no_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_stemmed = [[stemmer.stem(word) for word in doc] for doc in doc_no_stops]\n",
    "docs_stemmed = [[stemmer.stem(word) for word in doc] for doc in doc_no_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733\n"
     ]
    }
   ],
   "source": [
    "print(len(docs_lemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words='english', max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-8d08075b8815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount_vectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_lemmed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1031\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    327\u001b[0m                                                tokenize)\n\u001b[1;32m    328\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 329\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "count_vectorized = count_vect.fit_transform(docs_lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(count_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = count_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emergence': 1618,\n",
       " 'china': 880,\n",
       " 'global': 2113,\n",
       " 'economic': 1571,\n",
       " 'power': 3401,\n",
       " 'important': 2348,\n",
       " 'developments': 1404,\n",
       " 'recent': 3695,\n",
       " 'decades': 1282,\n",
       " 'past': 3263,\n",
       " 'years': 4987,\n",
       " 'chinese': 881,\n",
       " 'economy': 1577,\n",
       " 'achieved': 247,\n",
       " 'growth': 2167,\n",
       " 'rate': 3653,\n",
       " 'averaging': 569,\n",
       " 'nearly': 3023,\n",
       " '10': 9,\n",
       " 'percent': 3289,\n",
       " 'year': 4986,\n",
       " 'resulting': 3901,\n",
       " 'output': 3198,\n",
       " 'person': 3313,\n",
       " 'overall': 3207,\n",
       " 'size': 4185,\n",
       " 'today': 4629,\n",
       " 'fourth': 2024,\n",
       " 'largest': 2650,\n",
       " 'world': 4965,\n",
       " 'terms': 4547,\n",
       " 'gross': 2157,\n",
       " 'domestic': 1510,\n",
       " 'product': 3505,\n",
       " 'gdp': 2090,\n",
       " 'current': 1235,\n",
       " 'exchange': 1778,\n",
       " 'rates': 3655,\n",
       " 'second': 4038,\n",
       " 'adjustments': 291,\n",
       " 'differences': 1418,\n",
       " 'purchasing': 3598,\n",
       " 'national': 3013,\n",
       " 'currencies': 1233,\n",
       " 'strong': 4374,\n",
       " 'performance': 3296,\n",
       " 'resulted': 3900,\n",
       " 'improved': 2359,\n",
       " 'living': 2756,\n",
       " 'standards': 4295,\n",
       " 'people': 3286,\n",
       " 'estimates': 1724,\n",
       " '200': 77,\n",
       " 'million': 2928,\n",
       " 'brought': 728,\n",
       " 'poverty': 3399,\n",
       " 'reforms': 3739,\n",
       " 'began': 620,\n",
       " '1978': 53,\n",
       " '2004': 83,\n",
       " 'life': 2723,\n",
       " 'reached': 3663,\n",
       " 'fallen': 1886,\n",
       " '26': 111,\n",
       " '000': 0,\n",
       " 'live': 2753,\n",
       " 'literacy': 2750,\n",
       " 'aged': 331,\n",
       " '90': 181,\n",
       " 'remarkable': 3798,\n",
       " 'nonetheless': 3054,\n",
       " 'measures': 2880,\n",
       " 'remains': 3797,\n",
       " 'developing': 1402,\n",
       " 'nation': 3012,\n",
       " 'particular': 3249,\n",
       " 'urban': 4800,\n",
       " 'centers': 829,\n",
       " 'typical': 4716,\n",
       " 'modern': 2966,\n",
       " 'society': 4213,\n",
       " 'average': 566,\n",
       " 'household': 2282,\n",
       " 'incomes': 2379,\n",
       " 'consumption': 1104,\n",
       " 'remain': 3793,\n",
       " 'quite': 3634,\n",
       " 'low': 2785,\n",
       " 'rural': 3989,\n",
       " 'areas': 469,\n",
       " 'faces': 1863,\n",
       " 'double': 1517,\n",
       " 'challenge': 845,\n",
       " 'sustaining': 4478,\n",
       " 'high': 2233,\n",
       " 'stable': 4282,\n",
       " 'stimulating': 4336,\n",
       " 'development': 1403,\n",
       " 'parts': 3257,\n",
       " 'country': 1187,\n",
       " 'shared': 4114,\n",
       " 'fully': 2058,\n",
       " 'boom': 677,\n",
       " 'remarks': 3800,\n",
       " 'like': 2727,\n",
       " 'offer': 3118,\n",
       " 'thoughts': 4598,\n",
       " 'continue': 1118,\n",
       " 'promote': 3537,\n",
       " 'welfare': 4920,\n",
       " 'progress': 3523,\n",
       " 'markets': 2845,\n",
       " 'agree': 346,\n",
       " 'determinant': 1390,\n",
       " 'level': 2708,\n",
       " 'productivity': 3508,\n",
       " 'worker': 4958,\n",
       " 'score': 4030,\n",
       " 'record': 3707,\n",
       " 'excellent': 1769,\n",
       " 'open': 3142,\n",
       " 'policy': 3366,\n",
       " '1989': 63,\n",
       " 'employed': 1629,\n",
       " 'grew': 2156,\n",
       " 'estimated': 1723,\n",
       " '1990': 64,\n",
       " '2005': 84,\n",
       " 'impressive': 2357,\n",
       " 'factors': 1873,\n",
       " 'contributed': 1133,\n",
       " 'including': 2375,\n",
       " 'capital': 785,\n",
       " 'investment': 2537,\n",
       " 'increasing': 2390,\n",
       " 'openness': 3146,\n",
       " 'trade': 4650,\n",
       " 'strengthening': 4359,\n",
       " 'educational': 1584,\n",
       " 'view': 4851,\n",
       " 'single': 4180,\n",
       " 'cause': 810,\n",
       " 'ongoing': 3138,\n",
       " 'expansion': 1803,\n",
       " 'moved': 2993,\n",
       " 'gradually': 2143,\n",
       " 'steadily': 4318,\n",
       " 'away': 580,\n",
       " 'central': 830,\n",
       " 'planning': 3347,\n",
       " 'greater': 2150,\n",
       " 'reliance': 3784,\n",
       " 'prices': 3466,\n",
       " 'determined': 1394,\n",
       " 'market': 2843,\n",
       " 'production': 3506,\n",
       " 'directed': 1437,\n",
       " 'state': 4305,\n",
       " 'government': 2136,\n",
       " 'reduced': 3720,\n",
       " 'direct': 1436,\n",
       " 'intervention': 2525,\n",
       " 'owned': 3219,\n",
       " 'enterprises': 1680,\n",
       " 'process': 3496,\n",
       " 'allowing': 372,\n",
       " 'scope': 4029,\n",
       " 'forces': 1990,\n",
       " '1999': 74,\n",
       " 'according': 233,\n",
       " 'estimate': 1722,\n",
       " '95': 185,\n",
       " 'retail': 3905,\n",
       " 'business': 754,\n",
       " '80': 171,\n",
       " 'agricultural': 350,\n",
       " 'commodities': 971,\n",
       " 'goods': 2128,\n",
       " 'conducted': 1042,\n",
       " 'substantial': 4405,\n",
       " 'experience': 1816,\n",
       " 'shown': 4146,\n",
       " 'economies': 1574,\n",
       " 'early': 1556,\n",
       " 'stages': 4286,\n",
       " 'complex': 1006,\n",
       " 'managed': 2823,\n",
       " 'effectively': 1588,\n",
       " 'centralized': 831,\n",
       " 'basis': 615,\n",
       " 'set': 4095,\n",
       " 'free': 2043,\n",
       " 'competitive': 997,\n",
       " 'serve': 4084,\n",
       " 'critical': 1222,\n",
       " 'functions': 2061,\n",
       " 'information': 2435,\n",
       " 'supply': 4447,\n",
       " 'demand': 1334,\n",
       " 'conditions': 1040,\n",
       " 'relative': 3775,\n",
       " 'specific': 4246,\n",
       " 'services': 4090,\n",
       " 'resources': 3870,\n",
       " 'productive': 3507,\n",
       " 'uses': 4809,\n",
       " 'providing': 3576,\n",
       " 'incentives': 2368,\n",
       " 'engage': 1655,\n",
       " 'cost': 1173,\n",
       " 'reduction': 3723,\n",
       " 'innovation': 2451,\n",
       " 'activities': 262,\n",
       " 'labor': 2637,\n",
       " 'particularly': 3250,\n",
       " 'sustained': 4477,\n",
       " 'area': 468,\n",
       " 'notably': 3071,\n",
       " 'reducing': 3722,\n",
       " 'barriers': 608,\n",
       " 'movement': 2994,\n",
       " 'workers': 4959,\n",
       " 'regions': 3748,\n",
       " 'sectors': 4044,\n",
       " 'firms': 1953,\n",
       " 'flexibility': 1963,\n",
       " 'determination': 1392,\n",
       " 'employment': 1634,\n",
       " 'wages': 4876,\n",
       " 'relatively': 3776,\n",
       " 'wage': 4875,\n",
       " 'jobs': 2574,\n",
       " 'higher': 2234,\n",
       " 'manufacturing': 2834,\n",
       " 'significant': 4164,\n",
       " 'source': 4236,\n",
       " 'decline': 1295,\n",
       " 'share': 4113,\n",
       " 'population': 3376,\n",
       " '1970': 45,\n",
       " '60': 149,\n",
       " 'indicates': 2404,\n",
       " 'scale': 4015,\n",
       " 'despite': 1381,\n",
       " 'shifts': 4129,\n",
       " 'large': 2647,\n",
       " 'example': 1764,\n",
       " 'related': 3769,\n",
       " 'industries': 2419,\n",
       " 'utilities': 4813,\n",
       " 'seven': 4103,\n",
       " 'times': 4622,\n",
       " 'considerable': 1077,\n",
       " 'portion': 3380,\n",
       " 'force': 1986,\n",
       " 'specifically': 4247,\n",
       " 'additional': 277,\n",
       " 'gains': 2076,\n",
       " 'realized': 3678,\n",
       " 'growing': 2164,\n",
       " 'small': 4203,\n",
       " 'medium': 2889,\n",
       " 'sized': 4186,\n",
       " 'emerging': 1620,\n",
       " 'engine': 1660,\n",
       " 'job': 2572,\n",
       " 'creation': 1208,\n",
       " 'united': 4768,\n",
       " 'states': 4309,\n",
       " 'help': 2226,\n",
       " 'create': 1204,\n",
       " 'dynamic': 1552,\n",
       " 'diversified': 1495,\n",
       " 'support': 4448,\n",
       " 'continuing': 1121,\n",
       " 'reduce': 3719,\n",
       " 'mobility': 2958,\n",
       " 'helping': 2229,\n",
       " 'obtain': 3101,\n",
       " 'education': 1583,\n",
       " 'training': 4662,\n",
       " 'need': 3027,\n",
       " 'new': 3043,\n",
       " 'occupations': 3109,\n",
       " 'encouraging': 1645,\n",
       " 'entrepreneurship': 1687,\n",
       " 'improvement': 2360,\n",
       " 'major': 2814,\n",
       " 'competition': 996,\n",
       " 'played': 3354,\n",
       " 'vital': 4862,\n",
       " 'role': 3967,\n",
       " 'opening': 3144,\n",
       " 'international': 2517,\n",
       " 'accelerated': 214,\n",
       " 'organization': 3172,\n",
       " '2001': 80,\n",
       " 'service': 4087,\n",
       " 'exposure': 1840,\n",
       " 'marketplace': 2844,\n",
       " 'forced': 1987,\n",
       " 'producers': 3502,\n",
       " 'partnership': 3255,\n",
       " 'foreign': 1998,\n",
       " 'increase': 2387,\n",
       " 'efficiency': 1592,\n",
       " 'improve': 2358,\n",
       " 'quality': 3622,\n",
       " 'globally': 2115,\n",
       " 'engaged': 1656,\n",
       " 'affiliates': 322,\n",
       " 'operating': 3150,\n",
       " 'helped': 2227,\n",
       " 'foster': 2017,\n",
       " 'introducing': 2530,\n",
       " 'technologies': 4535,\n",
       " 'techniques': 4533,\n",
       " 'enhancing': 1668,\n",
       " 'based': 611,\n",
       " 'results': 3902,\n",
       " 'energy': 1651,\n",
       " 'sector': 4043,\n",
       " 'presents': 3442,\n",
       " 'opportunity': 3157,\n",
       " 'know': 2625,\n",
       " 'appetite': 441,\n",
       " 'grown': 2165,\n",
       " 'rapidly': 3649,\n",
       " 'oil': 3132,\n",
       " 'risen': 3950,\n",
       " '50': 138,\n",
       " '2000': 78,\n",
       " 'agency': 333,\n",
       " 'usage': 4802,\n",
       " 'increased': 2388,\n",
       " '400': 127,\n",
       " 'day': 1267,\n",
       " '2006': 85,\n",
       " 'representing': 3835,\n",
       " 'half': 2182,\n",
       " 'rapid': 3648,\n",
       " 'use': 4803,\n",
       " 'reflects': 3737,\n",
       " 'intensive': 2494,\n",
       " 'pattern': 3267,\n",
       " 'pricing': 3467,\n",
       " 'remaining': 3796,\n",
       " 'price': 3464,\n",
       " 'controls': 1141,\n",
       " 'liberalization': 2718,\n",
       " 'sustainable': 4476,\n",
       " 'promoting': 3540,\n",
       " 'efficient': 1593,\n",
       " 'households': 2283,\n",
       " 'supplies': 4446,\n",
       " 'benefits': 638,\n",
       " 'obtained': 3102,\n",
       " 'larger': 2649,\n",
       " 'decisions': 1293,\n",
       " 'extraordinary': 1857,\n",
       " 'devoted': 1408,\n",
       " 'equipment': 1698,\n",
       " 'office': 3122,\n",
       " 'buildings': 743,\n",
       " 'partly': 3252,\n",
       " 'financed': 1940,\n",
       " 'saving': 4010,\n",
       " 'pace': 3224,\n",
       " 'raises': 3640,\n",
       " 'concerns': 1031,\n",
       " 'deployed': 1351,\n",
       " 'ways': 4897,\n",
       " 'analysts': 403,\n",
       " 'getting': 2106,\n",
       " 'adequate': 284,\n",
       " 'return': 3913,\n",
       " 'fixed': 1959,\n",
       " 'averaged': 567,\n",
       " '33': 119,\n",
       " 'annual': 421,\n",
       " 'rose': 3975,\n",
       " '40': 126,\n",
       " 'remained': 3795,\n",
       " 'suggesting': 4424,\n",
       " 'lower': 2786,\n",
       " 'comparisons': 990,\n",
       " 'drawn': 1534,\n",
       " 'asian': 495,\n",
       " 'countries': 1186,\n",
       " 'south': 4238,\n",
       " 'korea': 2632,\n",
       " 'japan': 2566,\n",
       " '1982': 58,\n",
       " '91': 182,\n",
       " 'period': 3300,\n",
       " '70': 160,\n",
       " 'relevant': 3781,\n",
       " '30': 115,\n",
       " 'heavy': 2220,\n",
       " 'investments': 2538,\n",
       " 'continued': 1119,\n",
       " 'signs': 4166,\n",
       " 'excess': 1774,\n",
       " 'capacity': 783,\n",
       " 'begun': 624,\n",
       " 'appear': 437,\n",
       " 'possible': 3391,\n",
       " 'indication': 2406,\n",
       " 'industry': 2420,\n",
       " 'appears': 439,\n",
       " '2002': 81,\n",
       " 'reach': 3662,\n",
       " '115': 18,\n",
       " 'metric': 2912,\n",
       " '11': 17,\n",
       " 'financial': 1942,\n",
       " 'play': 3353,\n",
       " 'allocation': 369,\n",
       " 'likely': 2729,\n",
       " 'returns': 3916,\n",
       " 'risk': 3953,\n",
       " 'exacerbate': 1752,\n",
       " 'problem': 3487,\n",
       " 'loans': 2759,\n",
       " 'contribute': 1132,\n",
       " 'future': 2070,\n",
       " 'instability': 2463,\n",
       " 'signals': 4161,\n",
       " 'takes': 4503,\n",
       " 'better': 644,\n",
       " 'account': 235,\n",
       " 'costs': 1175,\n",
       " 'inputs': 2455,\n",
       " 'complementary': 1001,\n",
       " 'scarce': 4018,\n",
       " '12': 19,\n",
       " 'taken': 4502,\n",
       " 'initial': 2444,\n",
       " 'steps': 4331,\n",
       " 'determining': 1396,\n",
       " 'beginning': 622,\n",
       " 'stock': 4339,\n",
       " 'strengthened': 4358,\n",
       " 'banking': 602,\n",
       " 'improving': 2363,\n",
       " 'supervision': 4438,\n",
       " 'confronting': 1058,\n",
       " 'enormous': 1671,\n",
       " 'institutions': 2470,\n",
       " 'partner': 3253,\n",
       " 'banks': 604,\n",
       " 'criteria': 1221,\n",
       " 'bank': 599,\n",
       " 'lending': 2698,\n",
       " 'trends': 4687,\n",
       " 'positive': 3388,\n",
       " 'great': 2149,\n",
       " 'deal': 1269,\n",
       " 'range': 3645,\n",
       " 'instruments': 2473,\n",
       " 'available': 565,\n",
       " 'savers': 4009,\n",
       " 'borrowers': 692,\n",
       " 'taking': 4504,\n",
       " 'ensure': 1673,\n",
       " 'credit': 1213,\n",
       " 'evaluation': 1735,\n",
       " 'extension': 1848,\n",
       " 'sound': 4234,\n",
       " 'finance': 1939,\n",
       " 'availability': 564,\n",
       " 'consumers': 1103,\n",
       " 'smaller': 4204,\n",
       " 'removing': 3809,\n",
       " 'eliminating': 1610,\n",
       " 'quantitative': 3624,\n",
       " 'administrative': 293,\n",
       " 'influence': 2429,\n",
       " 'composition': 1014,\n",
       " 'finally': 1938,\n",
       " 'require': 3843,\n",
       " 'appropriate': 459,\n",
       " 'institutional': 2469,\n",
       " 'foundation': 2020,\n",
       " 'function': 2059,\n",
       " 'defined': 1317,\n",
       " 'property': 3549,\n",
       " 'rights': 3946,\n",
       " 'intellectual': 2484,\n",
       " 'transparent': 4678,\n",
       " 'accounting': 239,\n",
       " 'good': 2125,\n",
       " 'corporate': 1161,\n",
       " 'governance': 2134,\n",
       " 'effective': 1587,\n",
       " 'supervisory': 4441,\n",
       " 'oversight': 3216,\n",
       " 'consistent': 1085,\n",
       " 'enforcement': 1654,\n",
       " 'contracts': 1128,\n",
       " 'rules': 3985,\n",
       " 'allow': 370,\n",
       " 'orderly': 3167,\n",
       " 'bankruptcy': 603,\n",
       " 'proceedings': 3494,\n",
       " 'focus': 1973,\n",
       " 'provide': 3571,\n",
       " 'long': 2769,\n",
       " 'run': 3986,\n",
       " 'macroeconomic': 2801,\n",
       " 'macroeconomics': 2802,\n",
       " 'believe': 629,\n",
       " 'benefit': 636,\n",
       " 'tools': 4637,\n",
       " 'managing': 2827,\n",
       " 'monetary': 2976,\n",
       " 'fiscal': 1954,\n",
       " 'policies': 3365,\n",
       " 'achieve': 246,\n",
       " 'inflation': 2426,\n",
       " 'stability': 4277,\n",
       " 'effects': 1590,\n",
       " 'temporary': 4539,\n",
       " 'fluctuations': 1972,\n",
       " 'consequently': 1072,\n",
       " 'banker': 600,\n",
       " '13': 22,\n",
       " 'capable': 782,\n",
       " 'work': 4956,\n",
       " 'extent': 1852,\n",
       " 'sufficiently': 4421,\n",
       " 'developed': 1400,\n",
       " 'authorities': 555,\n",
       " 'affect': 317,\n",
       " 'activity': 263,\n",
       " 'reasonably': 3682,\n",
       " 'predictable': 3417,\n",
       " 'way': 4896,\n",
       " 'reliable': 3783,\n",
       " 'transmission': 4675,\n",
       " 'mechanism': 2882,\n",
       " 'reason': 3680,\n",
       " 'reform': 3738,\n",
       " 'strengthen': 4357,\n",
       " 'sensitive': 4074,\n",
       " 'borrowing': 693,\n",
       " 'aggregate': 338,\n",
       " 'funds': 2068,\n",
       " 'principles': 3475,\n",
       " 'projects': 3531,\n",
       " 'expected': 1809,\n",
       " 'adjusted': 288,\n",
       " 'undertaken': 4744,\n",
       " 'slowing': 4200,\n",
       " 'borne': 688,\n",
       " 'promising': 3536,\n",
       " 'officials': 3127,\n",
       " 'make': 2816,\n",
       " 'judgments': 2593,\n",
       " 'effectiveness': 1589,\n",
       " 'enhanced': 1665,\n",
       " 'maintain': 2810,\n",
       " 'close': 918,\n",
       " 'link': 2739,\n",
       " 'dollar': 1508,\n",
       " 'presence': 3437,\n",
       " 'inflows': 2428,\n",
       " 'arising': 480,\n",
       " 'surplus': 4459,\n",
       " 'purchases': 3597,\n",
       " 'denominated': 1343,\n",
       " 'assets': 508,\n",
       " 'buy': 757,\n",
       " 'dollars': 1509,\n",
       " 'increases': 2389,\n",
       " 'money': 2978,\n",
       " 'result': 3899,\n",
       " 'unless': 4775,\n",
       " 'selling': 4067,\n",
       " 'bonds': 674,\n",
       " 'investors': 2540,\n",
       " 'primarily': 3468,\n",
       " 'commercial': 963,\n",
       " 'procedure': 3490,\n",
       " 'commonly': 974,\n",
       " 'referred': 3728,\n",
       " 'routinely': 3982,\n",
       " 'desired': 1380,\n",
       " 'possibly': 3392,\n",
       " 'leading': 2673,\n",
       " 'overheating': 3211,\n",
       " 'date': 1261,\n",
       " 'largely': 2648,\n",
       " 'successful': 4413,\n",
       " 'operations': 3153,\n",
       " 'continues': 1120,\n",
       " 'strategy': 4354,\n",
       " 'eventually': 1742,\n",
       " 'encounter': 1641,\n",
       " 'problems': 3489,\n",
       " 'maintained': 2811,\n",
       " 'near': 3022,\n",
       " 'levels': 2709,\n",
       " 'accordingly': 234,\n",
       " 'value': 4823,\n",
       " 'outstanding': 3205,\n",
       " 'grow': 2163,\n",
       " 'substitute': 4408,\n",
       " 'held': 2225,\n",
       " 'private': 3481,\n",
       " 'potentially': 3398,\n",
       " 'perception': 3291,\n",
       " 'point': 3362,\n",
       " 'fueled': 2055,\n",
       " 'expect': 1806,\n",
       " 'earn': 1557,\n",
       " 'ultimately': 4719,\n",
       " 'speculative': 4254,\n",
       " 'longer': 2770,\n",
       " 'term': 4545,\n",
       " 'restrictions': 3895,\n",
       " 'flow': 1969,\n",
       " 'limited': 2733,\n",
       " 'narrow': 3009,\n",
       " 'permitting': 3308,\n",
       " 'entirely': 1682,\n",
       " 'eliminate': 1608,\n",
       " 'stabilize': 4279,\n",
       " 'trigger': 4690,\n",
       " 'appreciation': 455,\n",
       " 'combined': 951,\n",
       " 'wider': 4932,\n",
       " 'trading': 4656,\n",
       " 'band': 598,\n",
       " 'ultimate': 4718,\n",
       " 'goal': 2118,\n",
       " 'independent': 2396,\n",
       " 'enhance': 1664,\n",
       " 'perspective': 3316,\n",
       " 'degree': 1322,\n",
       " 'short': 4136,\n",
       " 'political': 3370,\n",
       " 'ability': 196,\n",
       " 'course': 1192,\n",
       " 'accountable': 237,\n",
       " 'governments': 2138,\n",
       " 'flows': 1971,\n",
       " 'transition': 4672,\n",
       " 'domestically': 1511,\n",
       " 'led': 2684,\n",
       " 'component': 1011,\n",
       " 'exports': 1838,\n",
       " 'imports': 2351,\n",
       " 'currently': 1236,\n",
       " 'thirds': 4592,\n",
       " 'geographic': 2099,\n",
       " 'seen': 4059,\n",
       " 'compared': 987,\n",
       " 'gaining': 2075,\n",
       " 'membership': 2897,\n",
       " 'accounts': 240,\n",
       " 'called': 773,\n",
       " 'processing': 3498,\n",
       " 'components': 1012,\n",
       " 'materials': 2857,\n",
       " 'imported': 2350,\n",
       " 'research': 3850,\n",
       " 'promotes': 3539,\n",
       " '14': 23,\n",
       " 'supports': 4452,\n",
       " 'principle': 3474,\n",
       " 'comparative': 985,\n",
       " 'advantage': 306,\n",
       " 'intensity': 2493,\n",
       " 'transfers': 4669,\n",
       " 'technology': 4536,\n",
       " 'knowledge': 2627,\n",
       " 'proved': 3569,\n",
       " 'attracting': 543,\n",
       " 'billion': 651,\n",
       " '1986': 60,\n",
       " '72': 163,\n",
       " 'making': 2819,\n",
       " 'kingdom': 2623,\n",
       " '15': 24,\n",
       " 'bringing': 715,\n",
       " 'products': 3509,\n",
       " 'methods': 2911,\n",
       " '16': 26,\n",
       " 'securities': 4048,\n",
       " 'significantly': 4165,\n",
       " 'reaching': 3664,\n",
       " '22': 106,\n",
       " 'running': 3987,\n",
       " 'surpluses': 4460,\n",
       " 'external': 1853,\n",
       " 'caused': 811,\n",
       " 'remarkably': 3799,\n",
       " '17': 28,\n",
       " 'lend': 2695,\n",
       " 'follows': 1980,\n",
       " 'balance': 592,\n",
       " 'payments': 3273,\n",
       " 'net': 3038,\n",
       " 'abroad': 198,\n",
       " 'acquisition': 253,\n",
       " 'finances': 1941,\n",
       " 'foreigners': 1999,\n",
       " 'corresponding': 1171,\n",
       " 'depressing': 1360,\n",
       " 'look': 2771,\n",
       " 'deficits': 1315,\n",
       " 'contributing': 1135,\n",
       " 'imbalances': 2321,\n",
       " 'economists': 1576,\n",
       " 'policymakers': 3368,\n",
       " 'argued': 473,\n",
       " 'unsustainable': 4786,\n",
       " 'leadership': 2672,\n",
       " 'recognized': 3701,\n",
       " 'importance': 2347,\n",
       " 'achieving': 249,\n",
       " 'having': 2204,\n",
       " 'recently': 3696,\n",
       " 'included': 2373,\n",
       " 'objectives': 3092,\n",
       " 'plan': 3345,\n",
       " '2010': 91,\n",
       " 'balances': 594,\n",
       " 'extensive': 1850,\n",
       " 'participation': 3248,\n",
       " 'systems': 4494,\n",
       " 'purpose': 3600,\n",
       " 'home': 2262,\n",
       " 'producing': 3504,\n",
       " 'rest': 3886,\n",
       " '38': 124,\n",
       " '45': 133,\n",
       " 'comparison': 989,\n",
       " 'india': 2401,\n",
       " 'data': 1259,\n",
       " 'aimed': 357,\n",
       " 'clearly': 915,\n",
       " 'widely': 4929,\n",
       " '18': 29,\n",
       " 'putting': 3610,\n",
       " 'drive': 1538,\n",
       " 'decrease': 1300,\n",
       " 'vulnerability': 4873,\n",
       " 'concluded': 1033,\n",
       " '19': 31,\n",
       " 'situation': 4182,\n",
       " 'worsened': 4970,\n",
       " 'weighted': 4917,\n",
       " 'real': 3674,\n",
       " '20': 76,\n",
       " 'consumer': 1102,\n",
       " 'expensive': 1815,\n",
       " 'determine': 1393,\n",
       " 'currency': 1234,\n",
       " 'provides': 3575,\n",
       " 'exporting': 1837,\n",
       " 'induce': 2415,\n",
       " 'implicit': 2341,\n",
       " 'heavily': 2219,\n",
       " 'export': 1835,\n",
       " 'viability': 4846,\n",
       " 'depends': 1349,\n",
       " 'lead': 2669,\n",
       " 'helpful': 2228,\n",
       " 'probably': 3486,\n",
       " '21': 104,\n",
       " 'present': 3438,\n",
       " 'poor': 3373,\n",
       " 'social': 4211,\n",
       " 'safety': 3997,\n",
       " 'covered': 1195,\n",
       " 'health': 2212,\n",
       " 'insurance': 2475,\n",
       " 'pension': 3285,\n",
       " 'plans': 3348,\n",
       " 'case': 805,\n",
       " 'replace': 3821,\n",
       " 'pre': 3409,\n",
       " 'retirement': 3911,\n",
       " 'earnings': 1560,\n",
       " 'apply': 448,\n",
       " 'economically': 1572,\n",
       " 'active': 260,\n",
       " 'expenditures': 1812,\n",
       " 'local': 2760,\n",
       " 'relief': 3787,\n",
       " 'security': 4052,\n",
       " 'similar': 4167,\n",
       " 'income': 2378,\n",
       " 'absence': 202,\n",
       " 'stronger': 4375,\n",
       " 'save': 4008,\n",
       " 'protect': 3562,\n",
       " 'risks': 3956,\n",
       " 'unexpected': 4757,\n",
       " 'medical': 2887,\n",
       " 'expenses': 1814,\n",
       " 'old': 3134,\n",
       " 'age': 330,\n",
       " 'program': 3521,\n",
       " 'expanding': 1802,\n",
       " 'potential': 3397,\n",
       " 'raising': 3641,\n",
       " 'time': 4620,\n",
       " 'spending': 4259,\n",
       " 'types': 4715,\n",
       " 'raise': 3638,\n",
       " 'means': 2875,\n",
       " 'fund': 2062,\n",
       " 'required': 3844,\n",
       " 'pay': 3270,\n",
       " 'dividends': 1498,\n",
       " 'measure': 2877,\n",
       " 'understand': 4738,\n",
       " 'consideration': 1079,\n",
       " 'access': 222,\n",
       " 'mortgages': 2989,\n",
       " 'forms': 2008,\n",
       " 'noted': 3073,\n",
       " 'objective': 3091,\n",
       " 'recognizing': 3703,\n",
       " 'goals': 2119,\n",
       " 'greatly': 2152,\n",
       " 'facilitated': 1865,\n",
       " 'proposed': 3554,\n",
       " 'various': 4834,\n",
       " 'reductions': 3724,\n",
       " 'taxes': 4521,\n",
       " 'announced': 418,\n",
       " 'effort': 1595,\n",
       " 'obviously': 3105,\n",
       " 'judge': 2589,\n",
       " 'success': 4412,\n",
       " 'initiatives': 2448,\n",
       " 'efforts': 1596,\n",
       " 'constructive': 1100,\n",
       " 'discussed': 1458,\n",
       " 'turn': 4708,\n",
       " 'allocating': 368,\n",
       " 'continuation': 1117,\n",
       " 'healthy': 2213,\n",
       " 'keeping': 2610,\n",
       " 'principal': 3472,\n",
       " 'arises': 479,\n",
       " 'likelihood': 2728,\n",
       " 'efficiently': 1594,\n",
       " 'highest': 2235,\n",
       " 'slower': 4199,\n",
       " 'stress': 4362,\n",
       " 'addition': 276,\n",
       " 'constrained': 1094,\n",
       " 'lack': 2638,\n",
       " 'inhibit': 2443,\n",
       " 'lies': 2722,\n",
       " 'provision': 3577,\n",
       " 'expand': 1800,\n",
       " 'especially': 1713,\n",
       " 'hope': 2271,\n",
       " 'benefited': 637,\n",
       " 'boost': 680,\n",
       " 'purchase': 3595,\n",
       " 'intermediate': 2513,\n",
       " 'joined': 2579,\n",
       " 'doubled': 1518,\n",
       " 'variety': 4833,\n",
       " 'opportunities': 3156,\n",
       " 'industrial': 2417,\n",
       " 'does': 1505,\n",
       " 'patterns': 3268,\n",
       " 'change': 851,\n",
       " 'adversely': 309,\n",
       " 'affected': 318,\n",
       " 'preserving': 3444,\n",
       " 'broad': 718,\n",
       " 'relationship': 3773,\n",
       " 'gain': 2073,\n",
       " 'interactions': 2500,\n",
       " 'challenges': 847,\n",
       " 'exist': 1794,\n",
       " 'requiring': 3848,\n",
       " 'address': 279,\n",
       " 'environment': 1689,\n",
       " 'regarding': 3742,\n",
       " 'contribution': 1136,\n",
       " 'avoiding': 576,\n",
       " 'integration': 2482,\n",
       " 'come': 952,\n",
       " 'responsibility': 3883,\n",
       " 'spirit': 4265,\n",
       " 'cooperation': 1153,\n",
       " 'jonathan': 2583,\n",
       " 'complete': 1002,\n",
       " 'handbook': 2187,\n",
       " 'ed': 1578,\n",
       " 'perspectives': 3317,\n",
       " 'september': 4081,\n",
       " 'sheet': 4123,\n",
       " 'needs': 3029,\n",
       " 'york': 4993,\n",
       " 'prospects': 3560,\n",
       " 'washington': 4891,\n",
       " 'pp': 3404,\n",
       " '51': 140,\n",
       " '61': 151,\n",
       " 'steven': 4332,\n",
       " 'li': 2714,\n",
       " 'robust': 3963,\n",
       " 'equilibrium': 1697,\n",
       " 'imf': 2322,\n",
       " 'working': 4961,\n",
       " 'paper': 3234,\n",
       " '06': 5,\n",
       " 'october': 3115,\n",
       " 'jeffrey': 2569,\n",
       " 'david': 1265,\n",
       " 'romer': 3971,\n",
       " 'american': 388,\n",
       " 'review': 3926,\n",
       " 'vol': 4864,\n",
       " '89': 180,\n",
       " 'june': 2597,\n",
       " '99': 189,\n",
       " 'adjusting': 289,\n",
       " 'presented': 3440,\n",
       " '27': 112,\n",
       " 'goodfriend': 2126,\n",
       " 'framework': 2030,\n",
       " 'louis': 2784,\n",
       " 'profits': 3519,\n",
       " 'far': 1896,\n",
       " 'eastern': 1567,\n",
       " '39': 125,\n",
       " '43': 131,\n",
       " 'evolve': 1747,\n",
       " 'july': 2594,\n",
       " 'brookings': 726,\n",
       " 'institution': 2468,\n",
       " 'press': 3447,\n",
       " 'driven': 1539,\n",
       " 'path': 3264,\n",
       " 'economics': 1573,\n",
       " 'institute': 2467,\n",
       " 'bureau': 752,\n",
       " 'statistics': 4312,\n",
       " 'statistical': 4311,\n",
       " 'sachs': 3990,\n",
       " 'andrew': 413,\n",
       " '1995': 70,\n",
       " 'convergence': 1145,\n",
       " 'papers': 3235,\n",
       " '29': 114,\n",
       " '32': 118,\n",
       " 'measuring': 2881,\n",
       " 'line': 2736,\n",
       " 'profile': 3514,\n",
       " 'indicators': 2409,\n",
       " 'www': 4984,\n",
       " 'org': 3170,\n",
       " 'htm': 2288,\n",
       " 'harm': 2201,\n",
       " 'markus': 2846,\n",
       " 'eds': 1581,\n",
       " 'competing': 995,\n",
       " '100': 10,\n",
       " 'footnotes1': 1984,\n",
       " 'text2': 4566,\n",
       " 'text3': 4573,\n",
       " 'outlook': 3197,\n",
       " 'database': 1260,\n",
       " 'capita': 784,\n",
       " 'fifth': 1931,\n",
       " 'text4': 4574,\n",
       " 'calculated': 763,\n",
       " 'federal': 1912,\n",
       " 'reserve': 3852,\n",
       " 'board': 665,\n",
       " 'staff': 4284,\n",
       " 'using': 4810,\n",
       " 'difficult': 1424,\n",
       " 'figures': 1934,\n",
       " 'treated': 4683,\n",
       " 'method': 2908,\n",
       " 'introduced': 2529,\n",
       " 'break': 705,\n",
       " 'series': 4082,\n",
       " 'text5': 4575,\n",
       " 'chapter': 857,\n",
       " 'discussion': 1461,\n",
       " 'text6': 4576,\n",
       " '25': 109,\n",
       " 'text7': 4577,\n",
       " 'details': 1384,\n",
       " 'text8': 4578,\n",
       " 'calculations': 766,\n",
       " ...}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '01',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '08',\n",
       " '09',\n",
       " '10',\n",
       " '100',\n",
       " '101',\n",
       " '102',\n",
       " '104',\n",
       " '108',\n",
       " '109',\n",
       " '10th',\n",
       " '11',\n",
       " '115',\n",
       " '12',\n",
       " '120',\n",
       " '125',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '150',\n",
       " '16',\n",
       " '165',\n",
       " '17',\n",
       " '18',\n",
       " '1873',\n",
       " '19',\n",
       " '1907',\n",
       " '1913',\n",
       " '1929',\n",
       " '1930s',\n",
       " '1933',\n",
       " '195',\n",
       " '1950s',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1962',\n",
       " '1963',\n",
       " '1968',\n",
       " '1969',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1972',\n",
       " '1973',\n",
       " '1974',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1990',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '19th',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2000s',\n",
       " '2001',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '2007a',\n",
       " '2007b',\n",
       " '2008',\n",
       " '2009',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2015a',\n",
       " '2015b',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '20th',\n",
       " '21',\n",
       " '21st',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '250',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '300',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '40',\n",
       " '400',\n",
       " '401',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '50',\n",
       " '500',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '60',\n",
       " '600',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '70',\n",
       " '700',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '80',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '90',\n",
       " '91',\n",
       " '92',\n",
       " '93',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '_________',\n",
       " 'aaa',\n",
       " 'aaronson',\n",
       " 'abandoned',\n",
       " 'abate',\n",
       " 'abcp',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abroad',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'abs',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorbing',\n",
       " 'absorption',\n",
       " 'abstract',\n",
       " 'abusive',\n",
       " 'academic',\n",
       " 'academics',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accelerating',\n",
       " 'acceleration',\n",
       " 'accelerator',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'access',\n",
       " 'accessible',\n",
       " 'accommodate',\n",
       " 'accommodation',\n",
       " 'accommodative',\n",
       " 'accompanied',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accord',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'accumulated',\n",
       " 'accumulation',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'ach',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achieving',\n",
       " 'acknowledge',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquisition',\n",
       " 'acquisitions',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actors',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'acute',\n",
       " 'adam',\n",
       " 'adapt',\n",
       " 'adapted',\n",
       " 'adapting',\n",
       " 'add',\n",
       " 'added',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adequacy',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adjust',\n",
       " 'adjustable',\n",
       " 'adjusted',\n",
       " 'adjusting',\n",
       " 'adjustment',\n",
       " 'adjustments',\n",
       " 'administration',\n",
       " 'administrative',\n",
       " 'admittedly',\n",
       " 'adopt',\n",
       " 'adopted',\n",
       " 'adopting',\n",
       " 'adoption',\n",
       " 'adrian',\n",
       " 'adult',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advances',\n",
       " 'advancing',\n",
       " 'advantage',\n",
       " 'advantages',\n",
       " 'adverse',\n",
       " 'adversely',\n",
       " 'advice',\n",
       " 'advisers',\n",
       " 'advisory',\n",
       " 'advocate',\n",
       " 'advocated',\n",
       " 'advocates',\n",
       " 'affairs',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affects',\n",
       " 'affiliated',\n",
       " 'affiliates',\n",
       " 'affiliations',\n",
       " 'afford',\n",
       " 'affordability',\n",
       " 'affordable',\n",
       " 'african',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'aggregate',\n",
       " 'aggregates',\n",
       " 'aggregation',\n",
       " 'aggregators',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'aging',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreement',\n",
       " 'agreements',\n",
       " 'agricultural',\n",
       " 'ahead',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aided',\n",
       " 'aig',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'aims',\n",
       " 'alan',\n",
       " 'albeit',\n",
       " 'alert',\n",
       " 'align',\n",
       " 'aligned',\n",
       " 'alignment',\n",
       " 'alike',\n",
       " 'alleviate',\n",
       " 'allocate',\n",
       " 'allocating',\n",
       " 'allocation',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'alongside',\n",
       " 'alt',\n",
       " 'alter',\n",
       " 'altered',\n",
       " 'alternative',\n",
       " 'alternatively',\n",
       " 'alternatives',\n",
       " 'altogether',\n",
       " 'ama',\n",
       " 'ambitious',\n",
       " 'amended',\n",
       " 'amendment',\n",
       " 'amendments',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'amico',\n",
       " 'amid',\n",
       " 'aml',\n",
       " 'amortization',\n",
       " 'amounts',\n",
       " 'ample',\n",
       " 'amplified',\n",
       " 'amplify',\n",
       " 'amsterdam',\n",
       " 'anacostia',\n",
       " 'analogous',\n",
       " 'analyses',\n",
       " 'analysis',\n",
       " 'analysts',\n",
       " 'analytic',\n",
       " 'analytical',\n",
       " 'analyze',\n",
       " 'analyzed',\n",
       " 'analyzing',\n",
       " 'anchor',\n",
       " 'anchored',\n",
       " 'anchoring',\n",
       " 'andrei',\n",
       " 'andrew',\n",
       " 'anecdotal',\n",
       " 'angeles',\n",
       " 'anne',\n",
       " 'anniversary',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announcements',\n",
       " 'annual',\n",
       " 'annually',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'ante',\n",
       " 'anti',\n",
       " 'anticipate',\n",
       " 'anticipated',\n",
       " 'anticipates',\n",
       " 'anticipation',\n",
       " 'antitrust',\n",
       " 'apart',\n",
       " 'app',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appeared',\n",
       " 'appears',\n",
       " 'appendix',\n",
       " 'appetite',\n",
       " 'apple',\n",
       " 'applicable',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'applied',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'appointed',\n",
       " 'appraisal',\n",
       " 'appreciably',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciation',\n",
       " 'approach',\n",
       " 'approaches',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'appropriately',\n",
       " 'approval',\n",
       " 'approved',\n",
       " 'approximately',\n",
       " 'apps',\n",
       " 'april',\n",
       " 'arbitrage',\n",
       " 'architecture',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'arena',\n",
       " 'arguably',\n",
       " 'argue',\n",
       " 'argued',\n",
       " 'argues',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'arise',\n",
       " 'arisen',\n",
       " 'arises',\n",
       " 'arising',\n",
       " 'arm',\n",
       " 'arms',\n",
       " 'arose',\n",
       " 'arrangement',\n",
       " 'arrangements',\n",
       " 'array',\n",
       " 'arrc',\n",
       " 'arrows',\n",
       " 'art',\n",
       " 'arthur',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'artificial',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'aspect',\n",
       " 'aspects',\n",
       " 'assess',\n",
       " 'assessed',\n",
       " 'assessing',\n",
       " 'assessment',\n",
       " 'assessments',\n",
       " 'asset',\n",
       " 'assets',\n",
       " 'assigned',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'associated',\n",
       " 'association',\n",
       " 'assume',\n",
       " 'assumed',\n",
       " 'assumes',\n",
       " 'assuming',\n",
       " 'assumption',\n",
       " 'assumptions',\n",
       " 'assurance',\n",
       " 'assure',\n",
       " 'assured',\n",
       " 'asymmetric',\n",
       " 'asymmetry',\n",
       " 'athanasios',\n",
       " 'atlanta',\n",
       " 'attack',\n",
       " 'attacks',\n",
       " 'attainment',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attempts',\n",
       " 'attend',\n",
       " 'attendant',\n",
       " 'attended',\n",
       " 'attending',\n",
       " 'attention',\n",
       " 'attentive',\n",
       " 'attitudes',\n",
       " 'attract',\n",
       " 'attracted',\n",
       " 'attracting',\n",
       " 'attractive',\n",
       " 'attributable',\n",
       " 'attributed',\n",
       " 'attributes',\n",
       " 'auction',\n",
       " 'auctions',\n",
       " 'audience',\n",
       " 'audit',\n",
       " 'augmented',\n",
       " 'august',\n",
       " 'australia',\n",
       " 'authorities',\n",
       " 'authority',\n",
       " 'authorized',\n",
       " 'authors',\n",
       " 'auto',\n",
       " 'automated',\n",
       " 'automatic',\n",
       " 'automatically',\n",
       " 'autor',\n",
       " 'availability',\n",
       " 'available',\n",
       " 'average',\n",
       " 'averaged',\n",
       " 'averages',\n",
       " 'averaging',\n",
       " 'averse',\n",
       " 'aversion',\n",
       " 'avert',\n",
       " 'avery',\n",
       " 'avoid',\n",
       " 'avoided',\n",
       " 'avoiding',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'away',\n",
       " 'baby',\n",
       " 'backdrop',\n",
       " 'backed',\n",
       " 'background',\n",
       " 'backgrounds',\n",
       " 'backstop',\n",
       " 'backup',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'bagehot',\n",
       " 'bailout',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'balances',\n",
       " 'balancing',\n",
       " 'ball',\n",
       " 'baltimore',\n",
       " 'band',\n",
       " 'bank',\n",
       " 'banker',\n",
       " 'bankers',\n",
       " 'banking',\n",
       " 'bankruptcy',\n",
       " 'banks',\n",
       " 'banksâ',\n",
       " 'bankâ',\n",
       " 'bar',\n",
       " 'barriers',\n",
       " 'bars',\n",
       " 'base',\n",
       " 'based',\n",
       " 'basel',\n",
       " 'baseline',\n",
       " 'basic',\n",
       " 'basis',\n",
       " 'bba',\n",
       " 'bear',\n",
       " 'bearing',\n",
       " 'bears',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'begins',\n",
       " 'begun',\n",
       " 'behalf',\n",
       " 'behavior',\n",
       " 'belief',\n",
       " 'beliefs',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'believes',\n",
       " 'ben',\n",
       " 'benchmark',\n",
       " 'benchmarks',\n",
       " 'beneficial',\n",
       " 'benefit',\n",
       " 'benefited',\n",
       " 'benefits',\n",
       " 'benign',\n",
       " 'benjamin',\n",
       " 'bernanke',\n",
       " 'best',\n",
       " 'beth',\n",
       " 'better',\n",
       " 'beveridge',\n",
       " 'bhcs',\n",
       " 'bias',\n",
       " 'bid',\n",
       " 'big',\n",
       " 'bilateral',\n",
       " 'billion',\n",
       " 'bills',\n",
       " 'binding',\n",
       " 'bis',\n",
       " 'bit',\n",
       " 'black',\n",
       " 'blanchard',\n",
       " 'bliley',\n",
       " 'blinder',\n",
       " 'block',\n",
       " 'blog',\n",
       " 'bls',\n",
       " 'blue',\n",
       " 'blunt',\n",
       " 'board',\n",
       " 'boarddocs',\n",
       " 'boards',\n",
       " 'boardâ',\n",
       " 'bodies',\n",
       " 'body',\n",
       " 'bolster',\n",
       " 'bolstered',\n",
       " 'bond',\n",
       " 'bonds',\n",
       " 'book',\n",
       " 'books',\n",
       " 'boom',\n",
       " 'boomers',\n",
       " 'booms',\n",
       " 'boost',\n",
       " 'boosted',\n",
       " 'boosting',\n",
       " 'boosts',\n",
       " 'border',\n",
       " 'borders',\n",
       " 'borio',\n",
       " 'born',\n",
       " 'borne',\n",
       " 'borrow',\n",
       " 'borrowed',\n",
       " 'borrower',\n",
       " 'borrowers',\n",
       " 'borrowing',\n",
       " 'boston',\n",
       " 'bound',\n",
       " 'bouts',\n",
       " 'box',\n",
       " 'brainard',\n",
       " 'branch',\n",
       " 'branches',\n",
       " 'branching',\n",
       " 'brazil',\n",
       " 'breaches',\n",
       " 'breadth',\n",
       " 'break',\n",
       " 'breakdown',\n",
       " 'breakdowns',\n",
       " 'breaking',\n",
       " 'brexit',\n",
       " 'brian',\n",
       " 'bridge',\n",
       " 'brief',\n",
       " 'briefly',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'british',\n",
       " 'broad',\n",
       " 'broader',\n",
       " 'broadly',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'broker',\n",
       " 'brokerage',\n",
       " 'brokers',\n",
       " 'brookings',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'bruce',\n",
       " 'brunnermeier',\n",
       " 'brynjolfsson',\n",
       " 'bsa',\n",
       " 'bubble',\n",
       " 'bubbles',\n",
       " 'buck',\n",
       " 'budget',\n",
       " 'budgets',\n",
       " 'buffer',\n",
       " 'buffers',\n",
       " 'build',\n",
       " 'building',\n",
       " 'buildings',\n",
       " 'builds',\n",
       " 'buildup',\n",
       " 'built',\n",
       " 'bulk',\n",
       " 'bulletin',\n",
       " 'burden',\n",
       " 'burdens',\n",
       " 'burdensome',\n",
       " 'bureau',\n",
       " 'burst',\n",
       " 'business',\n",
       " 'businesses',\n",
       " 'bust',\n",
       " 'buy',\n",
       " 'buyers',\n",
       " 'buying',\n",
       " 'ca',\n",
       " 'cajner',\n",
       " 'calculate',\n",
       " 'calculated',\n",
       " 'calculating',\n",
       " 'calculation',\n",
       " 'calculations',\n",
       " 'calendar',\n",
       " 'calibrated',\n",
       " 'calibrating',\n",
       " 'calibration',\n",
       " 'calif',\n",
       " 'california',\n",
       " 'called',\n",
       " 'calls',\n",
       " 'cambridge',\n",
       " 'came',\n",
       " 'campbell',\n",
       " 'canada',\n",
       " 'cap',\n",
       " 'capabilities',\n",
       " 'capability',\n",
       " 'capable',\n",
       " 'capacity',\n",
       " 'capita',\n",
       " 'capital',\n",
       " 'capitalized',\n",
       " 'caps',\n",
       " 'capture',\n",
       " 'captured',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'career',\n",
       " 'careers',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'carnegie',\n",
       " 'carol',\n",
       " 'carolina',\n",
       " 'carried',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'cars',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cash',\n",
       " 'categories',\n",
       " 'category',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'causes',\n",
       " 'causing',\n",
       " 'caution',\n",
       " 'cautious',\n",
       " 'cautiously',\n",
       " 'cbo',\n",
       " 'ccar',\n",
       " 'ccp',\n",
       " 'ccps',\n",
       " 'ccyb',\n",
       " 'cdfi',\n",
       " 'cdfis',\n",
       " 'cdos',\n",
       " 'cds',\n",
       " 'census',\n",
       " 'center',\n",
       " 'centered',\n",
       " 'centers',\n",
       " 'central',\n",
       " 'centralized',\n",
       " 'centrally',\n",
       " 'centre',\n",
       " 'centuries',\n",
       " 'century',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'certainty',\n",
       " 'cfpb',\n",
       " 'cfr',\n",
       " 'chain',\n",
       " 'chains',\n",
       " 'chair',\n",
       " 'chairman',\n",
       " 'challenge',\n",
       " 'challenged',\n",
       " 'challenges',\n",
       " 'challenging',\n",
       " 'chance',\n",
       " 'chances',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'changing',\n",
       " 'channel',\n",
       " 'channels',\n",
       " 'chapter',\n",
       " 'character',\n",
       " 'characteristic',\n",
       " 'characteristics',\n",
       " 'characterized',\n",
       " 'charge',\n",
       " 'charged',\n",
       " 'charges',\n",
       " 'charles',\n",
       " 'chart',\n",
       " 'chartered',\n",
       " 'chase',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'checking',\n",
       " 'checks',\n",
       " 'chen',\n",
       " 'chicago',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'childhood',\n",
       " 'children',\n",
       " 'chile',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'chip',\n",
       " 'choice',\n",
       " 'choices',\n",
       " 'choose',\n",
       " 'choosing',\n",
       " 'chose',\n",
       " 'chosen',\n",
       " 'christina',\n",
       " 'christopher',\n",
       " 'circle',\n",
       " 'circumstance',\n",
       " 'circumstances',\n",
       " 'cited',\n",
       " 'cities',\n",
       " 'citizens',\n",
       " 'city',\n",
       " 'civil',\n",
       " 'claim',\n",
       " 'claims',\n",
       " 'clarida',\n",
       " 'clarify',\n",
       " 'clarifying',\n",
       " 'clarity',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'classic',\n",
       " 'classified',\n",
       " 'clear',\n",
       " 'cleared',\n",
       " 'clearer',\n",
       " 'clearing',\n",
       " 'clearinghouse',\n",
       " 'clearinghouses',\n",
       " 'clearly',\n",
       " 'cleveland',\n",
       " 'clients',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closely',\n",
       " 'closer',\n",
       " 'closing',\n",
       " 'club',\n",
       " 'cmbs',\n",
       " 'coalition',\n",
       " 'code',\n",
       " 'codes',\n",
       " 'coefficient',\n",
       " 'collaboration',\n",
       " 'collaborative',\n",
       " 'collapse',\n",
       " 'collapsed',\n",
       " 'collateral',\n",
       " 'collateralized',\n",
       " 'colleague',\n",
       " 'colleagues',\n",
       " 'collect',\n",
       " 'collected',\n",
       " 'collecting',\n",
       " 'collection',\n",
       " 'collective',\n",
       " 'collectively',\n",
       " 'college',\n",
       " 'colleges',\n",
       " 'collins',\n",
       " 'columbia',\n",
       " 'com',\n",
       " 'combat',\n",
       " 'combination',\n",
       " 'combine',\n",
       " 'combined',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'comfort',\n",
       " 'comfortable',\n",
       " 'coming',\n",
       " 'commend',\n",
       " 'commensurate',\n",
       " 'comment',\n",
       " 'commentators',\n",
       " 'comments',\n",
       " 'commerce',\n",
       " 'commercial',\n",
       " 'commission',\n",
       " 'commissions',\n",
       " 'commitment',\n",
       " 'commitments',\n",
       " 'committed',\n",
       " 'committee',\n",
       " 'committees',\n",
       " 'commodities',\n",
       " 'commodity',\n",
       " 'common',\n",
       " 'commonly',\n",
       " 'communicate',\n",
       " 'communicated',\n",
       " 'communicating',\n",
       " 'communication',\n",
       " 'communications',\n",
       " 'communities',\n",
       " 'community',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'comparable',\n",
       " 'comparative',\n",
       " 'compare',\n",
       " 'compared',\n",
       " 'comparing',\n",
       " 'comparison',\n",
       " 'comparisons',\n",
       " 'compelling',\n",
       " 'compensate',\n",
       " 'compensation',\n",
       " 'compete',\n",
       " 'competing',\n",
       " 'competition',\n",
       " 'competitive',\n",
       " 'competitors',\n",
       " 'complacency',\n",
       " ...]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidvect = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vectorized = tfidvect.fit_transform(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(733, 27767)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _minmax_mixin.argmin of <733x27767 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 618867 stored elements in Compressed Sparse Row format>>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "vocabulary_ not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-96282c4aa156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount_vectorized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: vocabulary_ not found"
     ]
    }
   ],
   "source": [
    "count_vectorized.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    '''\n",
    "    INPUT: string\n",
    "    OUTPUT: list of strings\n",
    "\n",
    "    Tokenize and stem/lemmatize the document.\n",
    "    '''\n",
    "    tokenized_docs= [doc.split() for doc in doc_list]\n",
    "    toc_doc_lower = [[word.lower() for word in doc] for doc in tokenized_docs]\n",
    "\n",
    "    cleaned_docs = [[remove_symbols(word, punct) for word in doc] \n",
    "                for doc in toc_doc_lower]\n",
    "    \n",
    "    stops = set(stopwords.words('english'))\n",
    "    doc_no_stops = [[word for word in doc if word not in stops] for doc in cleaned_docs]\n",
    "    \n",
    "    lemmer = WordNetLemmatizer()\n",
    "    docs_lemmed = [[lemmer.lemmatize(word) for word in doc] for doc in doc_no_stops]\n",
    "    return docs_lemmed\n",
    "    #return [lemmer.lemmatize(word) for word in word_tokenize(doc.lower())]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(word, symbol_set):\n",
    "    return ''.join(char for char in word \n",
    "                    if char not in symbol_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tokenize(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "733\n"
     ]
    }
   ],
   "source": [
    "#docs[44]\n",
    "print(type(docs))\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-c1638f7ed473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcount_vectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1031\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    327\u001b[0m                                                tokenize)\n\u001b[1;32m    328\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 329\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "count_vectorized = count_vect.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-c03c49f21f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtfidf_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtfidf_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1611\u001b[0m         \"\"\"\n\u001b[1;32m   1612\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1614\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1031\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    327\u001b[0m                                                tokenize)\n\u001b[1;32m    328\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 329\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(stop_words = 'english', max_features=5000)\n",
    "tfidf_v = tfidf_vect.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-36ca74c755d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#count_vect = CountVectorizer(stop_words='english', tokenizer = tokenize)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcount_vectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1031\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "#count_vect = CountVectorizer(stop_words='english', tokenizer = tokenize)\n",
    "count_vect = CountVectorizer(tokenizer = tokenize)\n",
    "count_vectorized = count_vect.fit_transform(doc_list).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-b38f407fe66f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtfidfvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtfidf_vectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1611\u001b[0m         \"\"\"\n\u001b[1;32m   1612\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1614\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1031\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "tfidfvect = TfidfVectorizer(tokenizer=tokenize, max_features=5000)\n",
    "tfidf_vectorized = tfidfvect.fit_transform(doc_list).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tfidfvect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words[300:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_values(lst, n, labels):\n",
    "    return [labels[i] for i in np.argsort(lst)[-1:-n-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-f37f1a958774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-147-51194811071a>\u001b[0m in \u001b[0;36mget_top_values\u001b[0;34m(lst, n, labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_top_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-147-51194811071a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_top_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "n=10\n",
    "a = get_top_values(tfidf_vectorized, n, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(733, 5000)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 by total tf-idf\n",
      "[',', '.', 'financial', 'bank', ')', '(', 'market', 'federal', 'rate', 'policy']\n"
     ]
    }
   ],
   "source": [
    "total = np.sum(tfidf_vectorized, axis=0)\n",
    "print(\"top 10 by total tf-idf\")\n",
    "print(get_top_values(total, 10, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable            Type                    Data/Info\n",
      "-----------------------------------------------------\n",
      "CountVectorizer     type                    <class 'sklearn.feature_e<...>on.text.CountVectorizer'>\n",
      "PorterStemmer       ABCMeta                 <class 'nltk.stem.porter.PorterStemmer'>\n",
      "RegexpTokenizer     ABCMeta                 <class 'nltk.tokenize.regexp.RegexpTokenizer'>\n",
      "SnowballStemmer     ABCMeta                 <class 'nltk.stem.snowball.SnowballStemmer'>\n",
      "TfidfVectorizer     type                    <class 'sklearn.feature_e<...>on.text.TfidfVectorizer'>\n",
      "WordNetLemmatizer   type                    <class 'nltk.stem.wordnet.WordNetLemmatizer'>\n",
      "cleaned_docs        list                    n=733\n",
      "count_vect          CountVectorizer         CountVectorizer(analyzer=<...>730950>, vocabulary=None)\n",
      "count_vectorized    ndarray                 733x44387: 32535671 elems, type `int64`, 260285368 bytes (248.22747039794922 Mb)\n",
      "cv                  ndarray                 733x5000: 3665000 elems, type `int64`, 29320000 bytes (27.96173095703125 Mb)\n",
      "df                  DataFrame                         date           <...>n\\n[733 rows x 5 columns]\n",
      "doc_list            list                    n=733\n",
      "doc_no_stops        list                    n=733\n",
      "docs_lemmed         list                    n=733\n",
      "docs_stemmed        list                    n=733\n",
      "get_top_values      function                <function get_top_values at 0x1a39730620>\n",
      "lemmer              WordNetLemmatizer       <WordNetLemmatizer>\n",
      "n                   int                     10\n",
      "np                  module                  <module 'numpy' from '/an<...>kages/numpy/__init__.py'>\n",
      "pd                  module                  <module 'pandas' from '/a<...>ages/pandas/__init__.py'>\n",
      "pickle              module                  <module 'pickle' from '/a<...>lib/python3.6/pickle.py'>\n",
      "punct               set                     {'>', '<', '^', '+', '?',<...> '*', '$', ']', '[', '}'}\n",
      "remove_symbols      function                <function remove_symbols at 0x1a39730a60>\n",
      "stemmer             SnowballStemmer         <nltk.stem.snowball.Snowb<...>r object at 0x1a32b467f0>\n",
      "stops               set                     {'as', 'i', 'aren', 'has'<...> 'he', 'shouldn', 'have'}\n",
      "stopwords           WordListCorpusReader    <WordListCorpusReader in <...>_data/corpora/stopwords'>\n",
      "string              module                  <module 'string' from '/a<...>lib/python3.6/string.py'>\n",
      "temp                list                    n=733\n",
      "tfidf_vectorized    ndarray                 733x5000: 3665000 elems, type `float64`, 29320000 bytes (27.96173095703125 Mb)\n",
      "tfidfvect           TfidfVectorizer         TfidfVectorizer(analyzer=<...>n        vocabulary=None)\n",
      "tfidvect            TfidfVectorizer         TfidfVectorizer(analyzer=<...>n        vocabulary=None)\n",
      "toc_doc_lower       list                    n=733\n",
      "tokenize            function                <function tokenize at 0x1a39730950>\n",
      "tokenized_docs      list                    n=733\n",
      "tokens              list                    n=733\n",
      "total               ndarray                 5000: 5000 elems, type `float64`, 40000 bytes\n",
      "word_tokenize       function                <function word_tokenize at 0x1a1843c2f0>\n",
      "words               list                    n=5000\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that sucks! Looks like punctuation has not been removed from the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidvect =TfidfVectorizer(lowercase=True, stop_words='english', max_features = 1000)\n",
    "tfidf_vectorized = tfidvect.fit_transform(doc_list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26767\n"
     ]
    }
   ],
   "source": [
    "stops  =tfidvect.stop_words_\n",
    "print(len(stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "vocab = tfidvect.vocabulary_\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_df = tfidvect.idf_\n",
    "inv_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_arr = tfidf_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0120277 , 0.01952326, 0.00467892, 0.00867747, 0.0050535 ,\n",
       "        0.01041692, 0.01029846, 0.01116975, 0.00570364, 0.00558488],\n",
       "       [0.        , 0.01840275, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01316085, 0.01344071, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.0350087 , 0.00757678, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01659447, 0.01325669, 0.01229286, 0.014318  ,\n",
       "        0.01475704, 0.02188383, 0.03164706, 0.01616001, 0.01582353],\n",
       "       [0.        , 0.00852611, 0.02043354, 0.00947396, 0.01103471,\n",
       "        0.01137307, 0.01124373, 0.        , 0.01245433, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.03138271, 0.01018803, 0.        , 0.        , 0.        ,\n",
       "        0.01358993, 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_arr[0:10,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the highest n values.\n",
    "    Return the labels for each of these indices.\n",
    "\n",
    "    e.g.\n",
    "    lst = [7, 3, 2, 4, 1]\n",
    "    n = 2\n",
    "    labels = [\"cat\", \"dog\", \"mouse\", \"pig\", \"rabbit\"]\n",
    "    output: [\"cat\", \"pig\"]\n",
    "    '''\n",
    "    return [labels[i] for i in np.argsort(lst)[-1:-n-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-92ad7471d3eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_sort_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtop_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_sort_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "test_sort_index = np.argsort(l_avg)\n",
    "test_sort_index\n",
    "top_10 = test_sort_index[-1:-11:-1]\n",
    "print(list(v_words(top_10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 by average tf-idf\n",
      "['saving', 'purchase', 'education', 'expect', 'loan', 'bound', 'governments', 'effective', 'second', 'efforts']\n"
     ]
    }
   ],
   "source": [
    "avg = np.sum(tfidf_arr, axis=0) / np.sum(tfidf_arr > 0, axis=0)\n",
    "print(\"top 10 by average tf-idf\")\n",
    "l_avg = list(avg)\n",
    "v_words = list(vocab.keys())\n",
    "print(get_top_values(l_avg, 10, v_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(l_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['china', 'global', 'economic', 'important', 'developments', 'recent', 'decades', 'past', 'years', 'economy', 'growth', 'rate', 'nearly', '10', 'percent', 'year', 'resulting', 'output', 'overall', 'size', 'today', 'fourth', 'largest', 'world', 'terms', 'gross', 'domestic', 'product', 'gdp', 'current', 'exchange', 'rates', 'second', 'adjustments', 'differences', 'national', 'strong', 'performance', 'improved', 'standards', 'people', 'estimates', 'million', 'reforms', 'began', '2004', 'life', 'fallen', '000', 'measures', 'remains', 'developing', 'nation', 'particular', 'average', 'household', 'consumption', 'remain', 'quite', 'low', 'areas', 'challenge', 'high', 'stable', 'development', 'country', 'fully', 'remarks', 'like', 'offer', 'continue', 'promote', 'progress', 'markets', 'level', 'productivity', 'open', 'policy', 'estimated', '2005', 'factors', 'contributed', 'including', 'capital', 'investment', 'increasing', 'trade', 'view', 'single', 'ongoing', 'expansion', 'moved', 'central', 'planning', 'greater', 'prices', 'market', 'production', 'state', 'government', 'reduced', 'direct', 'process', 'scope', '1999', 'according', 'estimate', 'business', 'goods', 'substantial', 'experience', 'shown', 'economies', 'early', 'complex', 'effectively', 'basis', 'set', 'serve', 'critical', 'information', 'supply', 'demand', 'conditions', 'relative', 'specific', 'services', 'resources', 'providing', 'incentives', 'cost', 'reduction', 'innovation', 'activities', 'labor', 'particularly', 'area', 'notably', 'reducing', 'workers', 'sectors', 'firms', 'employment', 'wages', 'relatively', 'wage', 'jobs', 'higher', 'significant', 'source', 'decline', 'share', 'population', 'scale', 'despite', 'large', 'example', 'related', 'times', 'considerable', 'force', 'additional', 'gains', 'growing', 'small', 'medium', 'emerging', 'job', 'united', 'states', 'help', 'create', 'support', 'reduce', 'education', 'need', 'new', 'improvement', 'major', 'competition', 'role', 'international', 'organization', '2001', 'service', 'foreign', 'increase', 'improve', 'quality', 'operating', 'helped', 'based', 'results', 'energy', 'sector', 'opportunity', 'know', 'rapidly', 'oil', '50', '2000', 'agency', 'increased', 'day', '2006', 'half', 'rapid', 'use', 'reflects', 'price', 'controls', 'sustainable', 'households', 'benefits', 'larger', 'decisions', 'office', 'saving', 'pace', 'concerns', 'ways', 'return', 'fixed', 'annual', 'rose', 'remained', 'lower', 'countries', 'period', 'relevant', '30', 'investments', 'continued', 'capacity', 'appear', 'possible', 'industry', 'appears', '2002', '11', 'financial', 'play', 'likely', 'returns', 'risk', 'problem', 'loans', 'future', 'better', 'account', 'costs', '12', 'taken', 'steps', 'beginning', 'stock', 'banking', 'improving', 'supervision', 'institutions', 'banks', 'bank', 'lending', 'trends', 'positive', 'great', 'deal', 'range', 'instruments', 'available', 'borrowers', 'taking', 'ensure', 'credit', 'sound', 'finance', 'availability', 'consumers', 'smaller', 'quantitative', 'influence', 'finally', 'require', 'appropriate', 'institutional', 'function', 'accounting', 'good', 'corporate', 'effective', 'supervisory', 'oversight', 'consistent', 'rules', 'allow', 'focus', 'provide', 'long', 'run', 'macroeconomic', 'believe', 'benefit', 'tools', 'monetary', 'fiscal', 'policies', 'achieve', 'inflation', 'stability', 'effects', '13', 'work', 'extent', 'developed', 'authorities', 'affect', 'activity', 'way', 'reason', 'reform', 'strengthen', 'borrowing', 'aggregate', 'funds', 'principles', 'expected', 'make', 'effectiveness', 'enhanced', 'maintain', 'close', 'dollar', 'purchases', 'assets', 'increases', 'money', 'result', 'bonds', 'investors', 'commercial', 'leading', 'largely', 'successful', 'operations', 'continues', 'strategy', 'problems', 'near', 'levels', 'value', 'held', 'private', 'potentially', 'point', 'expect', 'ultimately', 'longer', 'term', 'flow', 'limited', 'trading', 'goal', 'independent', 'perspective', 'degree', 'short', 'political', 'ability', 'course', 'governments', 'flows', 'led', 'exports', 'currently', 'seen', 'compared', 'accounts', 'called', 'research', '14', 'technology', 'knowledge', 'billion', 'making', '15', 'products', '16', 'securities', 'significantly', '17', 'lend', 'balance', 'payments', 'net', 'abroad', 'look', 'economists', 'policymakers', 'importance', 'having', 'recently', 'included', 'objectives', 'plan', '2010', 'participation', 'systems', 'home', 'data', 'clearly', '18', '19', 'situation', 'real', '20', 'consumer', 'currency', 'provides', 'lead', 'probably', 'present', 'social', 'safety', 'health', 'insurance', 'plans', 'case', 'pre', 'earnings', 'apply', 'local', 'security', 'similar', 'income', 'stronger', 'risks', 'program', 'potential', 'time', 'spending', 'types', 'raise', 'means', 'fund', 'required', 'pay', 'measure', 'understand', 'access', 'mortgages', 'noted', 'objective', 'goals', 'proposed', 'various', 'effort', 'success', 'efforts', 'discussed', 'turn', 'stress', 'addition', 'provision', 'especially', 'hope', 'purchase', 'variety', 'opportunities', 'industrial', 'does', 'change', 'affected', 'broad', 'relationship', 'challenges', 'address', 'environment', 'regarding', 'come', 'responsibility', 'september', 'sheet', 'needs', 'york', 'prospects', 'washington', 'pp', 'robust', 'working', 'paper', 'october', 'david', 'american', 'review', 'vol', 'june', 'framework', 'far', 'july', 'institution', 'press', 'path', 'economics', 'bureau', 'statistics', 'papers', 'line', 'indicators', 'www', 'text2', 'text3', 'outlook', 'text4', 'federal', 'reserve', 'board', 'staff', 'using', 'difficult', 'series', 'discussion', '25', 'sources', 'adverse', 'best', 'cash', 'issues', 'involved', 'provided', 'implementation', '2003', 'useful', 'analysis', 'alternative', 'approaches', 'used', 'november', 'text', 'conference', 'uncertainty', 'face', 'think', 'nature', 'tend', 'lessons', 'views', 'necessarily', 'reflect', 'members', 'committee', 'fomc', 'faced', 'going', 'understanding', 'expectations', 'bankers', 'typically', 'subject', 'issue', 'resource', 'historical', 'consider', 'just', 'late', 'fall', 'implications', 'known', 'changes', 'personal', 'index', 'hard', 'month', 'pressures', 'compensation', 'special', 'begin', 'hand', 'rising', 'release', 'suggests', 'total', 'studies', 'suggest', 'existing', 'certain', 'key', 'trend', 'figure', 'behavior', 'downward', 'cycle', 'later', 'questions', 'sales', 'survey', 'movements', 'general', 'associated', 'assessment', 'guidance', 'companies', 'number', 'wide', 'common', 'nominal', 'treasury', 'yields', 'premium', 'liquidity', 'forecast', 'needed', 'form', 'sense', 'question', 'stance', 'asset', 'structure', 'public', 'debt', 'equity', 'order', 'directly', 'say', 'premiums', 'underlying', 'house', 'previous', 'estate', 'doing', 'created', 'fundamental', 'rise', 'homes', 'dynamics', 'focused', 'model', 'evolution', 'effect', 'models', 'rule', 'possibility', 'simply', 'factor', 'concern', 'study', 'end', 'shocks', 'severe', 'little', 'simple', 'history', 'shift', 'slow', 'elevated', 'unemployment', 'explain', 'core', 'post', 'generally', 'makes', 'robert', 'ago', 'evidence', 'response', 'looking', 'conduct', 'actions', 'respond', 'given', 'certainly', 'approach', 'circumstances', '1990s', 'conclusion', 'gap', 'did', 'light', 'necessary', 'events', 'lines', 'control', 'maximum', 'loss', 'different', 'contrast', 'standard', 'bit', 'weak', 'zero', 'extended', 'management', 'event', 'requires', 'practice', 'projections', 'intended', 'primary', 'aspects', 'wealth', 'confidence', 'individuals', 'fed', 'decision', 'recognize', 'speech', 'thought', 'thank', 'michael', 'actual', 'sharp', 'journal', 'april', 'governors', 'december', 'european', 'march', 'january', 'john', 'groups', 'university', 'theory', 'functioning', 'mortgage', 'discuss', 'regulatory', 'proposals', 'basel', 'ii', 'cross', 'manage', 'customers', 'positions', 'supervisors', 'losses', 'minimum', 'requirements', 'tool', 'negative', 'consequences', 'advanced', 'discipline', 'systemic', 'examiners', 'holding', 'reasons', 'default', 'months', 'agencies', 'issued', 'meet', 'want', 'final', 'forward', 'soundness', 'chairman', 'bernanke', 'organizations', 'operational', 'august', 'proposal', 'develop', 'community', 'instead', 'mid', 'depository', 'residential', 'raised', 'regard', 'transparency', 'periods', 'earlier', 'encourage', 'identify', 'group', 'meeting', 'let', 'limit', 'established', 'individual', 'coming', 'practices', 'colleagues', 'closely', 'regulators', 'impact', 'able', 'regulation', 'regime', 'leverage', 'ratio', 'action', 'hold', 'counterparties', 'tax', 'assess', 'include', 'cases', 'slack', 'moderate', 'congress', 'report', 'place', 'roughly', 'housing', 'somewhat', 'declines', 'easing', 'monitor', 'statement', 'department', 'quarter', 'outcomes', 'decade', 'construction', 'family', 'points', 'reported', 'shows', 'normal', 'declined', 'experienced', 'outside', 'broader', 'following', 'reserves', 'scenario', 'forecasts', 'broadly', 'ahead', 'fact', 'women', 'sharply', 'improvements', 'said', 'volatility', 'participants', 'pressure', 'non', 'families', 'neutral', 'failure', 'clear', 'firm', 'quickly', 'businesses', 'recession', 'designed', 'mandate', 'similarly', 'clearing', 'dual', 'derivatives', 'target', 'yield', 'note', 'importantly', 'bond', 'recovery', '2007', 'tightening', 'changed', 'curve', 'accommodative', 'right', 'communication', 'authority', 'funding', 'context', 'deposit', 'discount', 'window', 'monitoring', 'resolution', 'deposits', 'substantially', 'big', 'fail', 'dealers', 'sponsored', 'incentive', 'traditional', 'school', 'globalization', 'independence', 'worth', 'instance', 'median', 'crises', 'start', 'increasingly', 'prior', 'payment', 'transactions', 'attention', 'loan', 'commitment', 'commodity', 'maturity', 'essential', 'intermediaries', 'infrastructure', 'financing', 'regulations', 'cambridge', 'city', 'pdf', 'changing', 'gov', 'america', 'law', 'february', 'percentage', 'ben', 'act', 'faster', 'requirement', 'structural', 'chicago', 'highly', 'savings', 'backed', 'liquid', 'legal', 'card', 'internal', 'mentioned', 'processes', 'budget', 'communities', 'senior', 'safe', 'protection', 'students', 'natural', 'sheets', 'spreads', 'margin', 'shock', 'council', 'lenders', 'programs', 'comprehensive', 'subprime', 'borrower', 'lender', 'neighborhoods', 'strategies', 'bear', 'properties', 'values', 'collateral', 'underwriting', 'cra', 'conventional', 'exposures', 'runs', 'compliance', 'avoid', 'testing', 'portfolio', 'assistance', 'sufficient', 'test', '2011', 'broker', 'taylor', 'vulnerabilities', 'sure', 'accommodation', 'center', '2008', 'mass', 'company', 'delivered', 'systemically', 'holdings', 'crisis', 'prudential', 'prevent', '2012', '2016', 'tests', 'nonbank', 'securitization', 'intermediation', 'frank', 'liabilities', 'bound', 'foreclosure', 'foreclosures', '2009', 'repo', 'macroprudential', 'shadow', '2013', 'dodd', '2018', '2014', '2015', '2017'])\n"
     ]
    }
   ],
   "source": [
    "print(xxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
