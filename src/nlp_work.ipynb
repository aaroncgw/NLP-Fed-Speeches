{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 733 entries, 0 to 734\n",
      "Data columns (total 5 columns):\n",
      "date       733 non-null datetime64[ns]\n",
      "speaker    733 non-null object\n",
      "title      733 non-null object\n",
      "link       733 non-null object\n",
      "text       733 non-null object\n",
      "dtypes: datetime64[ns](1), object(4)\n",
      "memory usage: 34.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pickle.load( open( \"../data/all_fed_speeches\", \"rb\" ) )\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial stab at tokenizing\n",
    "one_doc = df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = one_doc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5190"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-59133ab245b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvocab_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mvocab_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_documents' is not defined"
     ]
    }
   ],
   "source": [
    "''' FROM THE LECTURE \n",
    "vocab_set = set()\n",
    "for doc in tokenized_documents:\n",
    "    vocab_set.update(set(doc))\n",
    "vocabulary = sorted(vocab_set)\n",
    "\n",
    "X = np.zeros(shape=(len(corpus), len(vocabulary)))\n",
    "\n",
    "for i, doc in enumerate(tokenized_documents):\n",
    "    for word in doc:\n",
    "        j = vocabulary.index(word)\n",
    "        X[i,j] += 1\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/davidsmith/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/davidsmith/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/davidsmith/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 split each doc into a list of tokens\n",
    "tokenized_docs = [doc.split() for doc in corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: lowercase everything \n",
    "t_docs_lower = [[word.lower() for word in doc]\n",
    "               for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: remove punctuation\n",
    "punct = set(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(word, symbol_set):\n",
    "    return ''.join(char for char in word if char not in symbol_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_docs = [[remove_symbols(word, punct) for word in doc] for doc in t_docs_lower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'until', 'because', \"wasn't\", 'hadn', \"you'd\", 'through', \"won't\", 'was', 'further', 'down', 'are', 'as', \"don't\", 'don', 'shouldn', \"aren't\", 'did', 'more', \"she's\", 'wouldn', 'against', 'does', \"weren't\", 'whom', 'hers', 'with', 'between', \"didn't\", 'once', \"hasn't\", 'yourself', 'into', 'about', 'wasn', \"doesn't\", 'other', 'there', 'just', 'both', 'its', 'now', \"you're\", 'before', 'to', 'o', 'only', 'same', 'then', 'all', 'and', 'ain', 'some', 'where', 'but', 'of', \"that'll\", \"should've\", 'being', 'in', 'which', 'should', 'couldn', 'each', 'itself', 'out', \"shouldn't\", 'from', 'at', 'herself', 'be', 'your', 'she', 'd', 'few', 'my', 'own', 'he', 'his', 'why', 'weren', 'doing', 'will', 'ma', 'yourselves', 'i', 'most', 'no', 'aren', 't', 'do', 're', 'her', 'shan', 'so', 'here', \"isn't\", 'am', 'me', 'mightn', 'has', 'nor', 'above', 'yours', 'him', 'y', 'those', 'a', 'theirs', \"couldn't\", 'too', 'or', 'm', 'if', 've', 'such', 'our', 'over', 'very', \"mustn't\", 'not', 'haven', 'ours', 'after', 'they', 'were', 'have', 'by', 'the', 'during', 'them', 'when', 'their', \"haven't\", 'how', 'than', 'this', 'it', 'while', 'needn', \"wouldn't\", \"mightn't\", 'that', 'can', 'you', 's', 'below', 'themselves', 'himself', 'll', \"needn't\", \"you'll\", 'myself', 'off', \"you've\", 'an', 'we', 'up', 'what', 'been', 'on', 'these', 'again', 'doesn', 'mustn', 'won', 'ourselves', 'is', \"it's\", 'for', 'under', 'who', 'isn', \"shan't\", 'having', 'any', 'didn', \"hadn't\", 'hasn', 'had'}\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Remove stop words\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_no_stops = [[word for word in doc if word not in stop_words] for doc in cleaned_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(docs_no_stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Stemming and Lemmatization\n",
    "lemmer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_lemmatized = [[lemmer.lemmatize(word) for word in doc] for doc in docs_no_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_stemmed = [[stemmer.stem(word) for word in doc] for doc in docs_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_count_vectorizer(docs):\n",
    "    '''\n",
    "    Args:\n",
    "        docs (list of lists of strings): corpus\n",
    "    Returns:\n",
    "        X_count (numpy array): count vectors\n",
    "        vocab (list of strings): alphabetical list \n",
    "                                 of unique words\n",
    "    '''\n",
    "    vocab_set = set()\n",
    "    for doc in docs:\n",
    "        vocab_set.update(doc)\n",
    "\n",
    "    vocab = sorted(vocab_set)\n",
    "\n",
    "    X_count = np.zeros(shape=(len(docs), len(vocab)))\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        for word in doc:\n",
    "            j = vocab.index(word)\n",
    "            X_count[i,j] += 1\n",
    "    return X_count, vocab  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count, vocab = our_count_vectorizer(docs_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectors = pd.DataFrame(data = X_count, columns = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5190"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectors.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf = X_count / X_count.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectors = pd.DataFrame(data = X_tf, columns=vocab)\n",
    "tf_vectors.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency * Inverse Doc Frequency Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_count = np.zeros(shape=(len(vocab),))\n",
    "\n",
    "for i,word in enumerate(vocab):\n",
    "    for doc in docs_cleaned:\n",
    "        if word in doc:\n",
    "            doc_count[i] += 1\n",
    "\n",
    "doc_freq = doc_count/len(corpus)\n",
    "            \n",
    "doc_freq_series = pd.Series(data=doc_freq, index=vocab)\n",
    "doc_freq_series.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_doc_freq = np.log(1/doc_freq_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = np.log(1/doc_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = pd.DataFrame(data=X_tf*idf, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairwise_dist = squareform(pdist(poem_vec, metric='cosine'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFTERNOON LECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
